# -*- coding: utf-8 -*-
"""PandemicCampusSimulator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LxykhwT9k8bESs0zi_QY73kP8GMRaepH
"""

!pip3 install stable-baselines3[extra]

import gym
import time
from gym import spaces
from gym.utils import seeding
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
import matplotlib.pyplot as plt
import pygame
import random
from enum import Enum

# Agent constants
DEMOCRAT_MEAN_COMPLIANCE = .6
DEMOCRAT_STD_COMPLIANCE = .1
REPUBLICAN_MEAN_COMPLIANCE = .4
REPUBLICAN_STD_COMPLIANCE = .2

INFECTION_RESISTANCE_DAILY_DECAY = .01
DAYS_BETWEEN_ACTIONS = 14

INTERACTION_MIN = 1
INTERACTION_MAX = 10

POST_INFECTION_RESISTANCE = 0.8

# most viral days 4-6
DAYS_INFECTED_TO_VIRALITY_MEAN = {
  -1:0,
  0: 0.1,
  1: 0.2,
  2: 0.3,
  3: 0.4,
  4: 0.6,
  5: 0.8,
  6: 1.0,
  7: 0.6,
  8: 0.5,
  9: 0.4,
  10: 0.3,
  11: 0.2,
  12: 0.1,
  13: 0.05,
  14: 0.02,
}

OUTSIDE_INFECTION_PROB = 0.01
CHANCE_OF_DYING = 0.0011 # from 2021 mortality rate https://www.cdc.gov/mmwr/volumes/71/wr/mm7117e1.htm#:~:text=In%202021%2C%20COVID%2D19%20was,%E2%89%A585%20years%20(1%2C395.7).
MASK_MANDATE_INFECTIVITY_MULTIPLIER = 0.5 # "If 90% of individuals wear 50% efficacious masks, this decreases Infection Attack Rate by 54%"  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7852241/
SOCIAL_DISTANCING_INFECTIVITY_MULTIPLIER = 0.2 # "from Both experiments show that infection rates are reduced drastically when social distancing intervention is implemented between 80% to 100%." https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8197116/
CLOSED_BUILDINGS_INFECTIVITY_MULTIPLIER = 0.65 # most nonessential businesses closed had upper bound ~35% reduction rate, we use this upper bounda as we model closing all buildings https://www.science.org/doi/10.1126/science.abd9338
VIRALITY_STD = 0.05
DETECTION_PROB = 0.01

# INFECTIVITY_MANDATE_MULTIPLIER = .
9
MINIMUM_INTERACTIONS = 2

class Status(Enum):
  SUSCEPTIBLE = 1
  INFECTED = 2
  RECOVERED = 3
  DEAD = 4

class Regulation(Enum):
  NOTHING = 0
  MASK_MANDATE = 1
  SOCIAL_DISTANCING = 2
  CLOSE_BUILDINGS = 3
  QUARANTINE = 4

REGULATION_NAMES = {
    Regulation.NOTHING.value: "No Policy",
    Regulation.MASK_MANDATE.value: "Mask Mandate",
    Regulation.SOCIAL_DISTANCING.value: "Social Distancing",
    Regulation.CLOSE_BUILDINGS.value: "Close Buildings",
    Regulation.QUARANTINE.value: "Quarantine"
}

REGULATION_COLORS = {
    Regulation.NOTHING.value: "black",
    Regulation.MASK_MANDATE.value: "red",
    Regulation.SOCIAL_DISTANCING.value: "blue",
    Regulation.CLOSE_BUILDINGS.value: "green",
    Regulation.QUARANTINE.value: "purple"
}

import random
from enum import Enum

class Agent:
  def __init__(self, initial_state, initial_location):
    # TODO: Bound to 0-1
    self.compliance_level = random.normalvariate(DEMOCRAT_MEAN_COMPLIANCE, DEMOCRAT_STD_COMPLIANCE) if random.random() < 0.5 else random.normalvariate(REPUBLICAN_MEAN_COMPLIANCE, REPUBLICAN_STD_COMPLIANCE)
    # if complying with
    self.infection_susceptibility = random.random()
    self.infection_resistance = 0
    # self.interaction_rate_lambda = random.randint(INTERACTION_MIN, INTERACTION_MAX)
    self.interaction_rate = random.random()
    # enforce_social_distancing
    self.death_susceptibility = random.uniform(0.0, 0.001)
    self.just_recovered = False
    self.state = initial_state  # Status enum
    self.location = initial_location  # Starting location
    self.days_infected = 0
    self.days_recovered = 0
    self.is_dead = False
    self.move_to(initial_location)


  def mandate_effectiveness(self, mandate_infectivity_multiplier):
    mandate_effectiveness = 1
    if mandate_infectivity_multiplier == 1:
      return 1

    compliance_virality_multiplier = (1 - self.compliance_level) * .5
    return min(1, compliance_virality_multiplier + mandate_infectivity_multiplier)

    # if mandate_infectivity_multiplier != 1:  # Mandate active
    #   compliance_virality_multiplier = (1 - self.compliance_level) * .5
    #   mandate_effectiveness = min(1, compliance_virality_multiplier + mandate_infectivity_multiplier)


  def probability_infected(self, exposure_virality_multiplier, mandate_infectivity_multiplier):
    # If infection_resistance is >= 1, agent is immune
    # infectivity_mandate_multiplier = 1 if not infectivity_mandate_active else self.compliance_level * INFECTIVITY_MANDATE_MULTIPLIER
    return max(0,
                self.infection_susceptibility * (1 - self.infection_resistance) * exposure_virality_multiplier * self.mandate_effectiveness(mandate_infectivity_multiplier))


  def infection_virality(self, mandate_infectivity_multiplier):
    # infectivity_mandate_multiplier = 1 if not infectivity_mandate_active else self.compliance_level * INFECTIVITY_MANDATE_MULTIPLIER
    exposure_virality_mean = DAYS_INFECTED_TO_VIRALITY_MEAN[self.days_infected]
    exposure_virality = random.normalvariate(exposure_virality_mean, VIRALITY_STD)

    return exposure_virality * self.mandate_effectiveness(mandate_infectivity_multiplier)


  def move_to(self, new_location):
    self.location = new_location
    self.x = random.uniform(new_location.x, new_location.x + new_location.width)
    self.y = random.uniform(new_location.y, new_location.y + new_location.height)

  def is_exposed(self, exposure_virality_mean, mandate_infectivity_multiplier):  # Add a chance of getting sick
    if self.state != Status.SUSCEPTIBLE:
      return

    exposure_virality = random.normalvariate(exposure_virality_mean, VIRALITY_STD)

    # mandate_effectiveness = 1
    # if mandate_infectivity_multiplier != 1:  # Mandate active
    #   compliance_virality_multiplier = (1 - self.compliance_level) * .5
    #   mandate_effectiveness = min(1, compliance_virality_multiplier + mandate_infectivity_multiplier)

    if random.random() < self.probability_infected(exposure_virality, mandate_infectivity_multiplier):
      self.state = Status.INFECTED
      self.days_infected = -1

  def get_draw_color(self):
    if self.state == Status.INFECTED:
      if self.days_infected <= 3:
         return (255, 165, 0)
      return (255, 0, 0)
    if self.state == Status.RECOVERED:
      return (128, 0, 255)
    if self.state == Status.DEAD:
      return (128, 0, 0)
    return (0, 0, 255)

  def day_ends(self, action):  # Add exponential chance of recovering, chance of dying?
    # Handle recovery and death if infected
    self.infection_resistance = max(0, self.infection_resistance - INFECTION_RESISTANCE_DAILY_DECAY)
    if self.state == Status.INFECTED:
      self.days_infected += 1
      if (self.days_infected > 7 and random.random() < 1/7) or self.days_infected >= 14:
        self.state = Status.RECOVERED
        self.infection_resistance = POST_INFECTION_RESISTANCE
        self.days_infected = 0
      else:
        if random.random() < self.death_susceptibility:
          self.state = Status.DEAD
          self.is_dead = True
          self.x = random.uniform(30, 35)
          self.y = random.uniform(30, 35)
    elif self.state == Status.RECOVERED:
      self.days_recovered += 1
      if self.days_recovered == 14:
        self.state = Status.SUSCEPTIBLE
        self.days_recovered = 0
    elif self.state == Status.SUSCEPTIBLE:
      discount = 1
      if action == Regulation.CLOSE_BUILDINGS.value:
        discount = 0.001
      if random.random() < OUTSIDE_INFECTION_PROB * discount:
        self.state = Status.INFECTED
        self.days_infected = -1

  def get_num_interactions(self, people_in_building):
    return min(
        people_in_building,
        MINIMUM_INTERACTIONS + np.random.poisson(max(0, (people_in_building - MINIMUM_INTERACTIONS) * self.interaction_rate))
        )

  # def get_num_interactions_with_compliance(self, people_in_building, infectivity_mandate_active=False):
  #   interactions = random.poisson(self.interaction_rate_lambda)
  #   if infectivity_mandate_active:
  #     interactions = int(interactions * (1 - self.compliance_level))

  #   return interactions


class Building:
  def __init__(self,
               x, y, width, height,
               building_id,
               is_open,
               interactivity_rate,
               essential_institution):
    self.x = x
    self.y = y
    self.width = width
    self.height = height
    self.building_id = building_id
    self.is_open = is_open
    self.interactivity_rate = interactivity_rate
    self.essential_institution = essential_institution

  def open(self):
    self.is_open = True

  def close(self):
    self.is_open = False

  def get_draw_info(self):
      return self.x, self.y, self.width, self.height

  # Handle all the people in the same space interacting
  def handle_interactions(self, agents, mandate_infectivity_multiplier):
    for agent in agents:
      if agent.state == Status.INFECTED:
        exposure_virality = agent.infection_virality(mandate_infectivity_multiplier)
        exposed_individuals = random.sample(agents, agent.get_num_interactions(len(agents)))
        for exposed_individual in exposed_individuals:
          exposed_individual.is_exposed(exposure_virality, mandate_infectivity_multiplier)

class PandemicCampusEnv(gym.Env):
    """Custom Environment for simulating a pandemic on a college campus."""

    def __init__(self, num_actions,
                 num_observations, num_agents,
                 prob_infected,
                 num_buildings,
                 max_time,
                 window_width,
                 window_height):
        super(PandemicCampusEnv, self).__init__()

        # Define action and observation space
        # Assume 'n' actions (policies) and 'm' observations (state representation)
        pygame.init()
        self.initialize_base_statistics()
        self.screen = pygame.display.set_mode((window_height, window_width))
        self.max_time = max_time
        self.font = pygame.font.Font(None, 24)
        self.action_space = spaces.Discrete(num_actions)
        self.observation_space = spaces.Box(low=0, high=1, shape=(num_observations,), dtype=np.float32)
        self.prob_infected = prob_infected
        self.num_agents = num_agents
        self.action_enacted = 0
        self.buildings = []
        self.initialize_buildings(window_width, window_height, num_buildings)
        self.initialize_agents(num_agents)
        self.infectivity_multiplier = 1

    def initialize_base_statistics(self):
      self.infection_rate = 0.3
      # self.interaction_prob = 0.25
      self.num_infected = 0
      self.current_time = 0
      self.counter_x = 10
      self.counter_y = 10
      self.num_dead = 0
      self.plot_times = []
      self.plot_infections = []
      self.plot_policies = []

    def initialize_agents(self, num_agents):
      self.agents = []
      for _ in range(num_agents):
        if np.random.rand() < self.prob_infected:
          self.num_infected += 1
          self.agents.append(Agent(initial_state=Status.INFECTED, initial_location=np.random.choice(self.buildings)))
        else:
          self.agents.append(Agent(initial_state=Status.SUSCEPTIBLE, initial_location=np.random.choice(self.buildings)))

    def is_overlapping(self, building1, building2):
      # Check if building1 overlaps with building2
      if (building1.x < building2.x + building2.width and
          building1.x + building1.width > building2.x and
          building1.y < building2.y + building2.height and
          building1.height + building1.y > building2.y):
          return True
      return False

    def seed(self, seed=None):
        # Seed the random number generator
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def initialize_buildings(self, window_width, window_height, num_buildings):
      max_width = 100  # Maximum width of a building
      max_height = 100  # Maximum height of a building
      for i in range(num_buildings):
        placed = False
        while not placed:
            width = random.randint(50, max_width)
            height = random.randint(50, max_height)
            x = random.randint(0, window_height - height)
            y = random.randint(0, window_width - width)

            new_building = Building(x, y, width, height,
                                    building_id=str(i),
                                    is_open=True,
                                    interactivity_rate=random.uniform(0, 1),
                                    essential_institution=False)

            # Check for overlaps with existing buildings
            if not any(self.is_overlapping(new_building, b) for b in self.buildings):
                self.buildings.append(new_building)
                placed = True
      num_essential_institutions = max(1, int(0.3 * len(self.buildings)))
      essential_buildings = random.sample(self.buildings, num_essential_institutions)
      for building in essential_buildings:
        building.essential_institution = True

    def step(self, action):
      for building in self.buildings:
        building.is_open = True
      # Apply the chosen action's effects
      if action == Regulation.NOTHING.value:
        self.infectivity_multiplier = 1
      elif action == Regulation.MASK_MANDATE.value:
        self.infectivity_multiplier = MASK_MANDATE_INFECTIVITY_MULTIPLIER
      elif action == Regulation.SOCIAL_DISTANCING.value:
        self.infectivity_multiplier = SOCIAL_DISTANCING_INFECTIVITY_MULTIPLIER
        # self.enforce_social_distancing(distance_factor=0.5)
      elif action == Regulation.CLOSE_BUILDINGS.value:
        self.infectivity_multiplier = CLOSED_BUILDINGS_INFECTIVITY_MULTIPLIER
        self.close_buildings()
      elif action == Regulation.QUARANTINE.value:
        self.infectivity_multiplier = CLOSED_BUILDINGS_INFECTIVITY_MULTIPLIER
        # self.enforce_social_distancing(distance_factor=0.5)
      self.action_enacted = action
      reward = 0
      for i in range(DAYS_BETWEEN_ACTIONS):
        # Move agents between buildings
        self.move_agents()
        pre_step_dead = self.num_dead
        pre_step_infections = self.num_infected
        for building in self.buildings:
          building_agents = [agent for agent in self.agents if agent.location == building]
          building.handle_interactions(building_agents, self.infectivity_multiplier)
        self.handle_end_of_day()
        self.num_dead = self.count_dead()
        num_died = self.num_dead - pre_step_dead
        # Calculate reward (negative for new infections, positive for healthy agents)
        reward += self.calculate_reward(self.num_infected - pre_step_infections, num_died, action)

        # Increment the current time
        self.current_time += 1
        # Check if the episode is done (e.g., after a fixed time, or if certain conditions are met)
        done = self.check_done()
        if done:
          break

      info = {}


      return self.get_observation(), reward, done, info

    def handle_end_of_day(self):
      num_infected = 0
      for agent in self.agents:
        agent.day_ends(self.action_enacted)
        if agent.state == Status.INFECTED:
          num_infected += 1
      self.num_infected = num_infected
      self.update_plot()

    def get_observation(self):
      # Example: Observation could be a vector with the status of each agent
      observation = np.zeros(self.observation_space.shape)
      for i, agent in enumerate(self.agents):
          observation[i] = self.encode_agent_status(agent)
      # Add more details to the observation as needed
      return observation

    def encode_agent_status(self, agent):
      # Encode the agent's status into a numerical value
      return agent.state.value

    def reduce_infection_rate(self, mask_effectiveness):
      # Assuming mask_effectiveness is a factor by which the infection rate is reduced
      self.infection_rate *= (1 - mask_effectiveness)

    def count_dead(self):
      dead = 0
      for agent in self.agents:
        if agent.state == Status.DEAD:
          dead += 1
      return dead
    # def enforce_social_distancing(self, distance_factor):
    #   # Adjust interaction probability based on social distancing
    #   self.interaction_prob *= distance_factor

    def count_new_infections(self, new_infections):
      # Counts the number of agents who have been newly infected in this time step
      # This requires tracking infections over time, which could be done with an additional attribute
      return new_infections

    def count_healthy_agents(self):
      # Counts the number of agents who are not infected
      return sum(1 for agent in self.agents if agent.state != Status.INFECTED)

    def close_buildings(self):
      # Mark certain buildings as closed, reducing interactions in those areas
      for building in self.buildings:
        if not building.essential_institution:
          building.is_open = False

    # tune this
    def calculate_reward(self, new_infections, num_died, action):
      # Example: Negative reward for each new infection, positive for healthy agents
      new_infections = self.count_new_infections(new_infections)
      reward = -10 * new_infections # disincentivize each new infection to account for lifelong negative repercussions like long covid
      reward -= 1000000 * num_died
      if action == Regulation.CLOSE_BUILDINGS.value:
        reward -= 0
        # reward -= 70
        # reward -= 20
      elif action == Regulation.SOCIAL_DISTANCING.value:
        reward -= 0
        # reward -= 25
        # reward -= 5
      elif action == Regulation.MASK_MANDATE.value:
        reward -= 0
        # reward -= 35
        # reward -= 10
      elif action == Regulation.QUARANTINE.value:
        reward -= 0
        # reward -= 90
        # reward -= 25
      return reward

    def move_agents(self):
        # Move agents between buildings based on some logic
        agents_to_move = self.agents
        if self.action_enacted == Regulation.CLOSE_BUILDINGS.value:
          num_to_move = int(0.2 * self.num_agents)
          agents_to_move = random.sample(self.agents, num_to_move)
          for agent in self.agents:
            if not agent.location.is_open and not agent in agents_to_move:
              agents_to_move.append(agent)
        for agent in agents_to_move:
          if agent.state == Status.DEAD:
            continue
          agent.location = self.decide_new_location(agent)
          agent.move_to(agent.location)

    def decide_new_location(self, agent):
        # Implement logic to decide a new location for the agent
        # Could be random, based on agent's schedule, etc.
        open_buildings = [building for building in self.buildings if building.is_open]
        if agent.state == Status.INFECTED and self.action_enacted == Regulation.QUARANTINE.value:
          if random.random() < 1 - DETECTION_PROB:
            return open_buildings[0]
          open_buildings = open_buildings[1:]
        return np.random.choice(open_buildings)

    def check_done(self):
      # Example: End the episode if a certain time has passed or if specific conditions are met
      if self.current_time > self.max_time or self.is_everyone_dead():
          return True
      return False

    def is_everyone_dead(self):
      for agent in self.agents:
        if agent.state != Status.DEAD:
          return False
      return True

    def is_everyone_infected(self):
      for agent in self.agents:
        if agent.state != Status.INFECTED:
          return False
      return True

    def reset(self):
        # Reinitialize agents and buildings
        self.initialize_agents(self.num_agents)
        self.initialize_base_statistics()
        for building in self.buildings:
            building.is_open = True
        # Reset other environment state variables
        return self.get_observation()

    def render(self):
      # Example visualization
      self.screen.fill((255, 255, 255))
      day_counter_surface = self.font.render(f'Day: {self.current_time}, Number of Deaths: {self.num_dead}, Policy Enacted: {REGULATION_NAMES[self.action_enacted]}', True, (0, 0, 0))  # Black text for day counter
      self.screen.blit(day_counter_surface, (10, 10))
      for building in self.buildings:
        pygame.draw.rect(self.screen, (0, 255, 0), pygame.Rect(building.x, building.y, building.width, building.height))  # Green rectangles for buildings
        # Draw text labels for each building
        text_surface = self.font.render(f'Building {building.building_id}', True, (0, 0, 0))  # Black text for labels
        self.screen.blit(text_surface, (building.x, building.y))  # Position the label

      for agent in self.agents:
        pygame.draw.circle(self.screen, agent.get_draw_color(), (int(agent.x), int(agent.y)), 5)

      pygame.display.flip()

    def update_plot(self):
      self.plot_times.append(self.current_time)
      self.plot_infections.append(self.num_infected)
      self.plot_policies.append(self.action_enacted)

    def plot(self):
      fig, ax = plt.subplots()
      ax.set_xlabel('Time')
      ax.set_ylabel('Number of Infected')
      ax.set_title('Infection Over Time Based on Policy')
      for i in range(5):
        ax.scatter([], [], color=REGULATION_COLORS[i], label=REGULATION_NAMES[i])
      ax.legend()
      for i in range(len(self.plot_times)):
        ax.scatter(self.plot_times[i], self.plot_infections[i], color=REGULATION_COLORS[self.plot_policies[i]])
      plt.show()

    def close(self):
        pass

from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env

#     def __init__(self, num_actions,
                #  num_observations, num_agents,
                #  prob_infected,
                #  num_buildings,
                #  max_time,
                #  window_width,
                #  window_height):


env = PandemicCampusEnv(num_actions=5, num_observations=100, num_agents=100, prob_infected=0.1,

                        num_buildings=20, max_time=730, window_width=800, window_height=600)
# total_reward = 0
# while not env.check_done():
#   action = 4
#   obs, rewards, dones, info = env.step(action)
#   total_reward += rewards
# print(total_reward)
# env.plot()

# Vectorized environments allow for parallelism and faster training
# vec_env = make_vec_env(lambda: env, n_envs=4)

# model = PPO("MlpPolicy", vec_env, verbose=1)

# model.learn(total_timesteps=1000)

# model.save("ppo_pandemic_campus")

# env.close()

# learning block
vec_env = make_vec_env(lambda: env, n_envs=4)

model = PPO("MlpPolicy", vec_env, verbose=1)

for i in range(3):
  env.reset()
  model.learn(total_timesteps=730)

model.save("ppo_pandemic_campus")

env.close()
env.plot()

# # Print total rewards of learned policy
# obs = env.reset()
# total_reward = 0
# while not env.check_done():
#     # do model.predict (outputs action, feed into env.step) here to print the total_rewards
#     # obs, rewards, dones, info = env.step(action)
#     # total_reward += rewards
#   print(total_reward)

from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env

#     def __init__(self, num_actions,
                #  num_observations, num_agents,
                #  prob_infected,
                #  num_buildings,
                #  max_time,
                #  window_width,
                #  window_height):
# this is a simulation
for action in range(5):
  env = PandemicCampusEnv(num_actions=5, num_observations=100, num_agents=100, prob_infected=0.1,

                          num_buildings=20, max_time=730, window_width=800, window_height=600)
  total_reward = 0
  while not env.check_done():
    # action = 4
    obs, rewards, dones, info = env.step(action)
    total_reward += rewards
  print(total_reward)
  env.plot()

# Reward and plot of learned policy
obs = env.reset()
num_days_survived = 0
total_reward = 0
while not env.check_done():
    action, _states = model.predict(obs, deterministic=True)
    obs, rewards, dones, info = env.step(int(action))
    total_reward += rewards
print("total reward: ", total_reward)
env.plot()

# env = PandemicCampusEnv(4, 100, 100, 0.1, 20, 100, 800, 600)
# env.render()
# time.sleep(5)
# while True:
#    env.step(0)
#    env.render()
#    print(env.num_infected)
#    time.sleep(5)